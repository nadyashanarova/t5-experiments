{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38054a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.models import Unigram\n",
    "import transformers\n",
    "from transformers import T5Config, T5Tokenizer, T5ForConditionalGeneration\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "import string\n",
    "import os\n",
    "import random\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25f675d",
   "metadata": {},
   "source": [
    "# Создание посимвольного токенизатора:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5144b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(Unigram())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa48f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    " \n",
    "normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "tokenizer.normalizer = normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d81a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f18bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import UnigramTrainer\n",
    "\n",
    "trainer = UnigramTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b1f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f\"./dataset_{split}_eq.csv\" for split in [\"test\", \"train\"]]\n",
    "\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd9a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"./tokenizer-equations-all.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dea643",
   "metadata": {},
   "source": [
    "Далее из созданного файла *tokenizer-equations-all.json* были удалены \"лишние\" символы. Оставлены только те, которые были использованы для конкректной задачи (символы, используемые для квадратного уравнения)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a5c4f9",
   "metadata": {},
   "source": [
    "# Загрузка посимвольного кастомного токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f97b74",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PreTrainedTokenizerFast' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mPreTrainedTokenizerFast\u001b[49m(tokenizer_object\u001b[38;5;241m=\u001b[39m Tokenizer\u001b[38;5;241m.\u001b[39mfrom_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tokenizer-equations.json\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PreTrainedTokenizerFast' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(tokenizer_object= Tokenizer.from_file(\"./tokenizer-equations.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.padding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057f079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1fa205",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031691e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files={'train': 'dataset_train_eq.csv',\n",
    "                                'val': 'dataset_test_eq.csv'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c8c287",
   "metadata": {},
   "source": [
    "Переведем исходный датасет из квадратных уравнений в датасет из токенов, чтобы узнать максимальную и минимальную длину входной и выходной последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff55ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "res_train = pd.read_csv('dataset_train_eq.csv', delimiter=',')\n",
    "res_test = pd.read_csv('dataset_test_eq.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33c91a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([res_train, res_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d145488",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = result.rename(columns={'input': \"token_ids\", \"output\": \"labels\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "674a1571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, 'x': 10, 'D': 11, 'n': 12, 'a': 13, '+': 14, '-': 15, '*': 16, '/': 17, '^': 18, '=': 19, '.': 20, '(': 21, ')': 22, ';': 23}\n"
     ]
    }
   ],
   "source": [
    "dict_of_token = {'0' : 0, '1' : 1, '2' : 2, '3' : 3, '4' : 4, '5' : 5, '6' : 6, '7' : 7, '8' : 8, '9' : 9, \\\n",
    "       'x' : 10, 'D' : 11, 'n' : 12, 'a': 13, \\\n",
    "        '+' : 14, '-' : 15, '*': 16, '/': 17, '^': 18, '=': 19, '.' : 20, '(': 21, ')' : 22, \";\": 23}\n",
    "\n",
    "# Распечатать словарь\n",
    "\n",
    "print(dict_of_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76b02d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol2token = dict_of_token\n",
    "\n",
    "def tokenize(string):\n",
    "    return [symbol2token[c] for c in string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8904f002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "max_count_labels = []\n",
    "for token in (res['token_ids']):\n",
    "    max_count_labels.append((len(token)))\n",
    "\n",
    "print(max(max_count_labels))\n",
    "\n",
    "max_count_input = []\n",
    "for token in (res['labels']):\n",
    "    max_count_input.append((len(token)))\n",
    "\n",
    "print(max(max_count_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574098b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#возьмем с небольшим запасом\n",
    "max_input_length = 30\n",
    "max_target_length = 120\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs =  examples[\"input\"]\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        labels = tokenizer(examples[\"output\"], max_length=max_target_length, truncation=True, padding=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f61ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc2d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login 73ec644a1563460e6ce79991d1c959ea5e20b053"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de972c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics4token(eval_pred):\n",
    "    batch_size = 32\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds =  [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels =  [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    answer_accuracy = []\n",
    "    token_accuracy = []\n",
    "    num_correct, num_total = 0, 0\n",
    "    num_answer = 0\n",
    "    number_eq = 0\n",
    "    for p, l in zip(decoded_preds, decoded_labels):\n",
    "        text_pred = p.split(' ')\n",
    "        text_labels = l.split(' ')\n",
    "        m = min(len(text_pred), len(text_labels))\n",
    "        if np.array_equal(text_pred, text_labels):\n",
    "            num_answer += 1\n",
    "        for i, j in zip(text_pred, text_labels):\n",
    "            if i == j:\n",
    "                num_correct += 1\n",
    "        num_total += len(text_labels)\n",
    "        number_eq += 1\n",
    "    token_accuracy = num_correct / num_total\n",
    "    answer_accuracy = num_answer / number_eq\n",
    "    result = {'token_acc': token_accuracy, 'answer_acc': answer_accuracy}\n",
    "    result = {key: value for key, value in result.items()}\n",
    "    for key, value in result.items():\n",
    "        wandb.log({key: value})        \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07153d6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0077a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"T5-4-6\", entity=\"kronesine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc094688",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"T5-4-6\", entity=\"kronesine\")\n",
    "df_marks = pd.DataFrame()\n",
    "model_name = 'АДАМВ'\n",
    "for head in [4]:\n",
    "    #wandb.login()\n",
    "    for layer in [6]:\n",
    "        for lr_scheduler_type in ['linear']:\n",
    "                for lr in [1.4e-4, 1e-1]:\n",
    "                    for optimizer in ['adamw_torch', 'adafactor']:\n",
    "                        wandb.init(name=f\"{model_name}-head_{head}-layer_{layer}-optim_{optimizer}-lr_{lr}-lr_scheduler_type_{lr_scheduler_type}\", project=\"T5-base-4-2\", entity=\"kronesine\")\n",
    "                        config = T5Config(decoder_start_token_id=tokenizer.convert_tokens_to_ids(['[PAD]'])[0], vocab_size=37, num_layers=layer, num_heads=head, d_kv=64, d_ff=2048, dropout_rate=0.1, max_length=120)\n",
    "                        model = T5ForConditionalGeneration(config=config).to(device)\n",
    "                        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "                        batch_size = 32\n",
    "                        args = Seq2SeqTrainingArguments(\n",
    "                            f\"{model_name}-head_{head}-layer_{layer}-optim_{optimizer}-lr_{lr}-lr_scheduler_type_{lr_scheduler_type}\",\n",
    "                            overwrite_output_dir=True,\n",
    "                            logging_first_step = True,\n",
    "                            evaluation_strategy = \"epoch\",\n",
    "                            save_strategy=\"epoch\",\n",
    "                            learning_rate=lr,\n",
    "                            lr_scheduler_type=lr_scheduler_type,\n",
    "                            optim=optimizer, \n",
    "                            per_device_train_batch_size=batch_size,\n",
    "                            per_device_eval_batch_size=batch_size,\n",
    "                            weight_decay=0.01,\n",
    "                            save_total_limit=3,\n",
    "                            num_train_epochs=100,\n",
    "                            predict_with_generate=True,\n",
    "                            fp16=True,\n",
    "                            metric_for_best_model = \"token_acc\",\n",
    "                            greater_is_better=True,   \n",
    "                            load_best_model_at_end=True,\n",
    "                            report_to=\"wandb\")\n",
    "                        trainer = Seq2SeqTrainer(\n",
    "                            model,\n",
    "                            args,\n",
    "                            train_dataset=(tokenized_datasets[\"train\"]),\n",
    "                            eval_dataset=(tokenized_datasets[\"val\"]),\n",
    "                            data_collator=data_collator,\n",
    "                            tokenizer=tokenizer,\n",
    "                            compute_metrics=compute_metrics4token\n",
    "                        )\n",
    "                        trainer.train()\n",
    "                        print(f\"{model_name}-head_{head}-layer_{layer}-optim_{optimizer}-lr_{lr}-lr_scheduler_type_{lr_scheduler_type}\")\n",
    "                        trainer.save_model()\n",
    "                        out = trainer.evaluate()\n",
    "                        new_row = {'dict_metrics': [out], 'combination': f\"{model_name}-head_{head}-layer_{layer}-optim_{optimizer}-lr_{lr}-lr_scheduler_type_{lr_scheduler_type}\"}\n",
    "                        df_marks = df_marks.append(new_row, ignore_index=True)\n",
    "                        #wandb.finish()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fcb87e",
   "metadata": {},
   "source": [
    "**Best model with parameters:**\n",
    "\n",
    "head_4-layer_6-optim_adamw_torch-lr_0.00014-lr_scheduler_type_linear\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
