# NLP project: How does a Transformer solve a task by Shanarova Nadia

nadya.shanarova@gmail.com

__Goal:__ discover attention patterns in a trained model
__Tasks:__ 
- research visualization methods
- pretrain a seq2seq Transformer on a downstream task
- visualize and interpret attention patterns 


**Input:** T5 model, task source and target

**Output:** visual schemes, insights


### Basic solution: heatmap of attention maps

### Influence:
cutting-edge research with modern models
deep insights into neural attention

____

## Цель исследования

Обнаружить закономерности паттернов внимания у обученной модели.

## Мотивация

Механизм внимания (attention) – чрезвычайно распространенный метод в современных моделях глубокого обучения, который позволяет не только улучшить показатели эффективности приложений нейронного машинного перевода, но и используя, к примеру, модель Transformer, повысить скорость обучения. В настоящее время возрастает актуальность задачи изучения возможностей и повышения эффективности данной модели, и поэтому, в данной работе предлагается подробно рассмотреть на примере решения квадратных уравнений каким образом данная модель последовательно решает задачу.

## Постановка задачи и методика исследования

Была поставлена задача многоклассовой классификации с использованием модели T5 из библиотеки Hugging Face и последующей интерпретации и визуализации паттернов внимания. Из  входной последовательности, состоящей из строкового представления квадратного уравнения было необходимо  получить решение и ответ этого уравнения.


Вход: 7x^2+3556x+451612=0


Выход: D=(3556^2-4*7*451612)=0;x=(-3556-0)/(2*7)=-254;x=(-3556+0)/(2*7)=-254;x=-254;x=-254;


Модель T5 обучена с нуля на сгенерированном датасете в количестве 100 000 примеров и их решений в виде подробного описания получения дискриминанта и корней уравнения. Всего 50 % составили уравнения с двумя решениями, 30 % с одним решением и остальные 20 % – без решений. Данные для обучения и тестирования были поделены в соотношении 80 на 20. 


После процедуры fine-tuning рассматривались различные модификации просмотра карт внимания для выявления гипотез. Был проведен анализ карты каждой головы на каждом отдельном слое, которые показывают важность конкретных операций входной, выходной последовательностей и их взаимосвязи. Усредненные карты внимания использовались для статистики, а именно какие операции чаще всего использовались. И, так называемая, суммарная карта: число слоев*число входных токенов, которая показывает важность каждого токена на каждом слое по всем головам.

## Результаты

В рамках данного исследования был сгенерирован датасет в виде квадратных уравнений и их решений, поставлена задача sequence to sequence и получение attention maps. Был проведен fune-tuning T5 и выбор лучших параметров модели, при которых качество правильно предсказанных символов составило 87 %, и качество полностью решенных уравнений 37 %. Далее были построены усредненные карты внимания по всем головам и слоям. Кроме этого, были получены визуальные схемы и сделаны выводы при интерпретации паттернов внимания. 

## Выводы

В ходе выполнения работы с помощью карт внимания были определены 4 явления:
Четкие диагонали на первых слоях появляются ввиду того, что модель  определяет контекст на каком-то определенно заданном расстоянии. 
Размытые диагонали – уже на ближнем расстоянии.
Вертикальные полосы. Модель свои знания о том, как решать квадратные уравнения определенным образом складывает в специальные символы ({x} , {^}, {=} и др.).
Разреженные квадраты. Чаще находятся на более глубоких слоях, когда есть четкая структура, откуда и каким образом модель определяет символы. Для этого она часто ссылается на предыдущие слои. Модель из них считывает, выводит коэффициенты и перезаписывает в известные ей символы.


Все явления переходят последовательно из одного в другое. Таким образом, можно сказать, что мы явления визуализируют различные уровни абстракции. Из третьего явления вытекает основная гипотеза, что специальные символы: {x}, {^}, {=} хранят в себе результаты вычислений и общие знания о решении задачи. Модель использует эти токены для записи и хранения информации.
